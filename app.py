import streamlit as st
from streamlit_extras.app_logo import add_logo
from st_pages import Page, Section, show_pages, add_page_title
from selenium import webdriver
from selenium.webdriver.common.by import By
from datetime import date
from bs4 import BeautifulSoup
import pandas as pd
import time
import openai
import altair as alt
import ast 
import os 

openai.api_key = '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'



add_selectbox = st.sidebar.selectbox(
    "ê²Œì„ì„ ì„ íƒí•˜ì„¸ìš”",
    ("ìš°ë§ˆë¬´ìŠ¤ë©”", "ì˜¤ë”˜", "ì•„í‚¤ì—ì´ì§€ì›Œ", "ê°€ë””ì–¸í…Œì¼ì¦ˆ")
)

# Using "with" notation
with st.sidebar:
    limit = st.radio(
        "ìµœê·¼ ë°ì´í„° ìˆ˜",
        ("10", "50")
    )
    board_type = st.radio(
        "ê²Œì‹œíŒ ì¢…ë¥˜",
        ("ê±´ì˜","ììœ ")
    )
    show = st.button("ê²°ê³¼ ë³´ê¸°")

if show:
    st.header(f'ğŸ¤–{add_selectbox} {board_type} ê²Œì‹œíŒ by Aibstract ')    

    col1, col2 = st.columns(2)
    if board_type=="ê±´ì˜":
        bdex= 1
    else:
        bdex=2


    mappings = {
        "ìš°ë§ˆë¬´ìŠ¤ë©”" : ["umamusume-kor", "ZaXF", "Z4oy", "https://img1.daumcdn.net/thumb/C151x151/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fcafeattach%2F1ZK1D%2F5c0477e8a6bfb94974136c978f5e2522dc17b29e"],
        "ì˜¤ë”˜" : ["odin", "DEIV", "D034", "https://img1.daumcdn.net/thumb/C151x151/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fcafeattach%2F1YvZ5%2Fabe49aa5aeb26dc71c157eef3bcf105bac519759"],
        "ì•„í‚¤ì—ì´ì§€ì›Œ" : ["ArcheAgeWar", "aaTV","ZmF6", "https://img1.daumcdn.net/thumb/C151x151/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fcafeattach%2F1ZOrf%2F26e8c4ce3b61a009d776c42eeadb7646e250f494"],
        "ì—ë²„ì†Œìš¸": ["Eversoul", "aCex", "Zkxr", "https://img1.daumcdn.net/thumb/C151x151/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fcafeattach%2F1ZLyF%2F2d4d47292e41f06e4163d2f20a9a66bca6394c3e"],
        "ê°€ë””ì–¸í…Œì¼ì¦ˆ": ["GuardianTales","AgPy","ARz6","https://img1.daumcdn.net/thumb/C151x151/?fname=https%3A%2F%2Ft1.daumcdn.net%2Fcafeattach%2F1YmAL%2F2fbe0cff9c7614a9f7750edb31a9735b37bd14a8"]
    }
    
    limit_map = {
        "10": "1",
        "20": "2",
        "30": "3",
        "40": "4",
        "50": "5"
    }

    @st.cache_data
    def get_raw_data(cafe_name, req_board_name, limit):
        driver = webdriver.Chrome()
        page = driver.get(f'https://cafe.daum.net/{cafe_name}/{req_board_name}')
        driver.switch_to.frame("down")
        driver.find_element(By.XPATH,'//*[@id="primaryContent"]/div/div[1]/div[2]/div[3]/div[1]/label').click()
        time.sleep(2)

        driver.find_element(By.XPATH,'//*[@id="primaryContent"]/div/div[1]/div[2]/div[3]/div[2]/a').click()
        time.sleep(2)

        driver.find_element(By.XPATH,f'//*[@id="primaryContent"]/div/div[5]/div[2]/div/div[{limit}]/a').click()
        time.sleep(2)


        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser') 
        profile = soup.select('#cafeProfileImage > img')[0]['src']


        post_titles = soup.find_all('a', {'class': 'txt_item'})
        article_num = soup.find_all('div',{'class':'wrap_num'})
        views = soup.find_all('span', {'class': 'tbl_txt_look'})
        rec_counts = soup.find_all('span',{'class':'tbl_txt_recommend'})
        dates = soup.find_all('td',{'class': 'td_date'})
        writer = soup.find_all('td',{'class': 'td_writer'})
        recomm =[]
        titles = []
        clicks = []
        article_id = []
        regdate = []
        author = []
        for ele in rec_counts:
            recomm.append(ele.get_text())

        for ele in views:
            clicks.append(ele.get_text())

        for ele in article_num:
            article_id.append(ele.get_text())

        for ele in post_titles:
            titles.append(ele.get_text())

        for e in writer:
            author.append(e.get_text())
        for e in dates:
            if str(e.get_text()).startswith('23.') == False:
                today = date.today().strftime("%y.%m.%d")
                regdate.append(today)
            else:
                regdate.append(e.get_text())
        data = { 'id': article_id, 'titles' : titles, 'recommendations': recomm, 'views': clicks , 'regdate': regdate, 'author':author}
        df = pd.DataFrame(data)
        df['views'] =  pd.to_numeric(df['views'])
        df['link'] = f'https://cafe.daum.net/{cafe_name}/{req_board_name}/' + df['id']
        return df


    def get_answers_from_gpt(command,temp):
        completion = openai.ChatCompletion.create (

        model = 'gpt-3.5-turbo-0613',
        messages = [{"role": "user", "content": command}],
        temperature=temp
        )
        return completion['choices'][0]['message']['content']


    @st.cache_data  
    def add_specific_contents(df):
        driver = webdriver.Chrome()
        contents = []
        for index, row in df.iterrows():
            url = df.iloc[index]['link']
            page = driver.get(url)
            driver.switch_to.frame("down")
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser') 
            if len(soup.select('#user_contents')) == 0 :
                contents.append('')
            else :
                texts = soup.select('#user_contents')[0].get_text()
                contents.append(texts.replace(u'\xa0', u' ').lstrip())
        df['contents']=contents
        return contents
    

    @st.cache_data
    def convert_df(df):
        # IMPORTANT: Cache the conversion to prevent computation on every rerun
        return df.to_csv().encode('utf-8')

    @st.cache_data
    def add_summary(df):
        summary = []
        for ele in df['contents']:
            command = f"""
                ì•„ë˜ì˜ ê±´ì˜ì‚¬í•­ì„ í•œë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜. 
                {ele}
                """
            completion = openai.ChatCompletion.create (

                model = 'gpt-3.5-turbo-0613',
                messages = [{"role": "user", "content": command}]
                
                )
            summary.append(completion['choices'][0]['message']['content'])
        df['summary'] = summary
        return df



    if add_selectbox in ("ì—ë²„ì†Œìš¸","ê°€ë””ì–¸í…Œì¼ì¦ˆ"):
        df = get_raw_data(mappings[add_selectbox][0],mappings[add_selectbox][bdex], limit_map[limit])
        add_specific_contents(df)
        add_summary(df)
        sent_cmd = f"""

                ì•„ë˜ ì „ë‹¬í•˜ëŠ” listëŠ” ê²Œì„ ê±´ì˜ ì œëª©ì¸ë°, ê°ê° sentiment analysisë¥¼ í•´ì„œ -1ê³¼ 1 ì‚¬ì´ì˜ ê°’ì„ ì„¤ëª… ì—†ì´ lengthê°€ 50ì¸ python listë¡œ ì„¤ëª€ ì—†ì´  ì „ë‹¬í•´ì¤˜. 

                {df['titles']}
                """
        sentiments = get_answers_from_gpt(sent_cmd,0.5)
        sentiments=ast.literal_eval(sentiments)
        print(sentiments)
        if len(sentiments) ==50:
            df['sentiments'] = sentiments
        df.to_csv(f'{mappings[add_selectbox][0]}-{mappings[add_selectbox][bdex]}.csv')

    else: 
        df = pd.read_csv(f'{mappings[add_selectbox][0]}-{mappings[add_selectbox][bdex]}.csv')

    add_logo(mappings[add_selectbox][3])
    # TOO SLOW 
    # if board_type == 'ììœ ':
    #     df = get_raw_data(mappings[add_selectbox][0],mappings[add_selectbox][2], limit_map[limit])
    #     add_specific_contents(df)
    #     add_summary(df)
    # else:
    #     df = get_raw_data(mappings[add_selectbox][0],mappings[add_selectbox][1], limit_map[limit])
    #     add_specific_contents(df)
    #     add_summary(df)
    # Streamlit UI 



    # sent_cmd = f"""

    # ì•„ë˜ ì „ë‹¬í•˜ëŠ” listëŠ” ê²Œì„ ê±´ì˜ ì œëª©ì´ì•¼.
    # ê°ê°ì˜ sentiment analysisë¥¼ í•´ì„œ -1ê³¼ 1 ì‚¬ì´ì˜ ê°’ì„ ì„¤ëª… ì—†ì´ lengthê°€ listì™€ ë™ì¼í•œ python listë¡œ ì „ë‹¬í•´ì¤˜. 

    # {df['titles']}
    # """
    # sentiments = get_answers_from_gpt(sent_cmd,0.3)
    # sentiments=ast.literal_eval(sentiments)
    # print(sentiments)
    # df['sentiments'] = sentiments

    date_count = df.groupby(df['regdate']).size().reset_index(name='Count')
    
    st.markdown(f"### {board_type} ê²Œì‹œíŒ 5ê°€ì§€ ì‹œì‚¬ì ")
    issues = get_answers_from_gpt(f"""
            ì•„ë˜ëŠ” ê±´ì˜í•¨ ë‚´ìš©ë“¤ì¸ë° ì£¼ìš” ê±´ì˜ ì‚¬í•­ 5ê°€ì§€ë¡œ ìš”ì•½í•˜ê³  ì‹œì‚¬ì ì„ ì•Œë ¤ì¤˜.  
            {df['titles']}
            """, 0.8)
    st.write(issues) 


    hot_issues = df.sort_values(by=['views','recommendations'],ascending=False)[:10]
    st.markdown("### ì¡°íšŒìˆ˜, ì¶”ì²œìˆ˜ ë†’ì€!")
    st.write(hot_issues[["titles","summary","link"]]) 


    with col1:
        st.markdown("### ë“±ë¡ ê¸€ í˜„í™©")
        st.line_chart(date_count, x='regdate',y='Count')

        

  

    with col2:
        st.markdown("### ë¹ˆë„ ë†’ì€ í‚¤ì›Œë“œ")
        keywords = get_answers_from_gpt(f"""
        ì•„ë˜ëŠ” ê²Œì„ ê±´ì˜í•¨ ë‚´ìš©ë“¤ì¸ë° ë¹ˆë„ê°€ ë†’ì€ ë‹¨ì–´ ë‹¤ì„¯ê°€ì§€ì™€ ë¹ˆë„ í¼ì„¼íŠ¸ë¥¼ ì•Œë ¤ì¤˜.
        {df['titles']}
        """, 0.8)
        st.write(keywords) 


    st.markdown("### ìµœê·¼ ë°ì´í„°")
    csv = convert_df(df)
    st.download_button(
        label="csvë¡œ ë‹¤ìš´ë°›ê¸°",
        data=csv,
        file_name=f'{mappings[add_selectbox][0]}-{mappings[add_selectbox][bdex]}.csv',
        mime='text/csv',
    )
    st.write(df[['id','titles','contents','regdate','summary','sentiments','author','link']])

    st.markdown("### Sentiment Analysis")
    sentiment_count = df.groupby(df['sentiments']).size().reset_index(name='Count')
    s_data = alt.Chart(df[["sentiments","regdate",'titles']]).mark_circle().encode(
        x='regdate', y='sentiments', size='sentiments', color='sentiments')
    st.altair_chart(s_data, use_container_width=True)

  
